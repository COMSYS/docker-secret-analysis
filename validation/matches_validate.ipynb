{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from hashlib import sha256\n",
    "import base64\n",
    "\n",
    "import yaml\n",
    "import re\n",
    "\n",
    "from pandarallel import pandarallel\n",
    "pandarallel.initialize(progress_bar=True, use_memory_fs=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Connect to SQL DATABASE!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions\n",
    "\n",
    "Functions used in the remainder of this notebook."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Match Functions\n",
    "\n",
    "Functions to match data using signature file. Required to (i) rematch matches from docker-analyzer and (ii) match on environment variables set in Dockerfiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../docker-analyzer/signatures/signatures.yaml\") as yaml_fd:\n",
    "  regexes = yaml.safe_load(yaml_fd)\n",
    "\n",
    "# this are multigroup regexes where the last group matches the actual secret\n",
    "multigroupRegexes = [\n",
    "  \"trufflehog_azure_oauth_client\",\n",
    "  \"trufflehog_azure_oauth_tenant\",\n",
    "  \"trufflehog_azure_old\",\n",
    "  \"trufflehog_heroku\",\n",
    "  \"trufflehog_digitaloceantoken\",\n",
    "  \"trufflehog_ibmclouduserkey\",\n",
    "  \"trufflehog_gitlab\",\n",
    "  \"trufflehog_currencycloud\",\n",
    "  \"trufflehog_openuv\",\n",
    "  \"trufflehog_netlify\",\n",
    "  \"trufflehog_coinbase\",\n",
    "  \"trufflehog_bitfenix\",\n",
    "  \"trufflehog_accuweather\",\n",
    "  \"trufflehog_wepay\",\n",
    "  \"trufflehog_tomtom\",\n",
    "  \"trufflehog_ticketmaster\",\n",
    "  \"trufflehog_paymongo\",\n",
    "  \"trufflehog_paymoapp\",\n",
    "  \"trufflehog_paydirtapp\",\n",
    "  \"trufflehog_loginradius\"\n",
    "]\n",
    "\n",
    "compiled_regexes = {}\n",
    "for t in regexes:\n",
    "  if t == \"hyperscan\":\n",
    "    for ns in regexes[t]:\n",
    "      for r in regexes[t][ns]:\n",
    "        name = '{}_{}'.format(ns, r)\n",
    "        regex = str.strip(regexes[t][ns][r])\n",
    "        compiled_regexes[name] = {}\n",
    "        compiled_regexes[name]['full'] = re.compile(regex)\n",
    "        if name in multigroupRegexes:\n",
    "          regexparts = re.match(\"\\([^\\(]*\\)({.+})*(?P<name>\\([^\\(]*\\))\\([^\\(]*\\)({[^\\{]+})*(?P<secret>.*)\", regex)\n",
    "          compiled_regexes[name]['name'] = re.compile(regexparts.group('name'))\n",
    "          compiled_regexes[name]['secret'] = re.compile(regexparts.group('secret'))\n",
    "\n",
    "def match(input, rule='*', retype='full', last=False):\n",
    "  regexes = {}\n",
    "  results = {}\n",
    "\n",
    "  if rule == '*':\n",
    "    regexes = compiled_regexes\n",
    "  else:\n",
    "    regexes[rule] = compiled_regexes[rule]\n",
    "\n",
    "  for r in regexes:\n",
    "    if retype in regexes[r]:\n",
    "      regex = regexes[r][retype]\n",
    "    else:\n",
    "      if retype == \"secret\":\n",
    "        regex = regexes[r][\"full\"]\n",
    "      else:\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "      m = regex.search(input)\n",
    "      if m:\n",
    "        if last and rule in multigroupRegexes:\n",
    "          results[r] = m.groups()[-1]\n",
    "        else:\n",
    "          results[r] = m.group(0)\n",
    "    except Exception as e:\n",
    "      print(\"Error matching {}: {}\".format(input, e))\n",
    "        \n",
    "  return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removesuffix(string, suffix):\n",
    "  if string.endswith(suffix):\n",
    "    return string[len(string)-len(suffix):]\n",
    "\n",
    "def rematch(row):\n",
    "  try:\n",
    "    d = row['data'].decode(errors=\"ignore\")\n",
    "  except Exception as e:\n",
    "    d = str(row['data'])\n",
    "  \n",
    "  res = match(d, rule=row['rule'], last=True)\n",
    "  if row['rule'] in res:\n",
    "    return res[row['rule']]\n",
    "  else:\n",
    "    print(\"Match could not be validated: {}\".format(d))\n",
    "    return \"\"\n",
    "\n",
    "def getPrePart(row):\n",
    "  try:\n",
    "    return removesuffix(row['data'].decode(), row['secret'])\n",
    "  except Exception as e:\n",
    "    return removesuffix(str(row['data']), row['secret'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import e, log\n",
    "\n",
    "# Calculate the Entropy of a string\n",
    "# Taken from https://gist.github.com/virtadpt/a129f94e47c113f983a1ee361f837eb8\n",
    "def stringEntropy(labels, base=None):\n",
    "  \"\"\" Computes entropy of label distribution. \"\"\"\n",
    "  if labels == np.NaN:\n",
    "    return 0\n",
    "  \n",
    "  l = list(labels)\n",
    "\n",
    "  n_labels = len(l)\n",
    "\n",
    "  if n_labels <= 1:\n",
    "    return 0\n",
    "\n",
    "  value,counts = np.unique(l, return_counts=True)\n",
    "  probs = counts / n_labels\n",
    "  n_classes = np.count_nonzero(probs)\n",
    "\n",
    "  if n_classes <= 1:\n",
    "    return 0\n",
    "\n",
    "  ent = 0.\n",
    "\n",
    "  # Compute entropy\n",
    "  base = e if base is None else base\n",
    "  for i in probs:\n",
    "    ent -= i * log(i, base)\n",
    "\n",
    "  return ent"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing functions\n",
    "\n",
    "Functions to parse matches, e.g., private keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def tryParseJSON(row):\n",
    "    try:\n",
    "        return json.loads(row['data'].decode(errors=\"ignore\"))\n",
    "    except Exception as e:\n",
    "        return {}\n",
    "\n",
    "# We consider the private key in the Google Cloud Secret as actual secret\n",
    "def gcpGetSecret(row):\n",
    "    if row[\"parsed\"] and \"private_key\" in row[\"parsed\"]:\n",
    "        return row[\"parsed\"][\"private_key\"]\n",
    "    else:\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xml.dom import minidom\n",
    "from Crypto.Util import number\n",
    "from Crypto.Util.asn1 import DerSequence\n",
    "from Crypto.PublicKey import RSA, DSA\n",
    "from base64 import standard_b64encode, b64decode\n",
    "\n",
    "def GetLong(nodelist):\n",
    "   rc = []\n",
    "   for node in nodelist:\n",
    "      if node.nodeType == node.TEXT_NODE:\n",
    "         rc.append(node.data)\n",
    "   string = ''.join(rc) \n",
    "   return number.bytes_to_long(b64decode(string))\n",
    "\n",
    "def xmlGetChildNodesIfOccurs(xml, key, id):\n",
    "    try:\n",
    "        tmp = xml.getElementsByTagName(key)\n",
    "    except Exception as e:\n",
    "        return None\n",
    "        \n",
    "    if len(tmp) > id:\n",
    "        return tmp[id].childNodes\n",
    "    \n",
    "    return None\n",
    "\n",
    "regexes = [\"comsys_xmlrsakey\", \"comsys_xmldsakey\", \"comsys_xmleckey\"]\n",
    "cregexes = [compiled_regexes[r]['full'] for r in regexes]\n",
    "def privKeyPEM(xmlPrivateKey):\n",
    "    result = []\n",
    "    for c in cregexes:\n",
    "        for x in c.finditer(xmlPrivateKey):\n",
    "            try:\n",
    "                xmlParsed = minidom.parseString(x[0])\n",
    "                if xmlParsed:\n",
    "                    if \"RSA\" in xmlPrivateKey:\n",
    "                        params = []\n",
    "                        modulus = GetLong(xmlGetChildNodesIfOccurs(xmlParsed, 'Modulus', 0))\n",
    "                        exponent = GetLong(xmlGetChildNodesIfOccurs(xmlParsed, 'Exponent', 0))\n",
    "                        if modulus == None or exponent == None:\n",
    "                            continue\n",
    "                        params.extend([modulus, exponent])\n",
    "\n",
    "                        d = GetLong(xmlGetChildNodesIfOccurs(xmlParsed, 'D', 0))\n",
    "                        if d != None:\n",
    "                            params.append(d)\n",
    "                            p = GetLong(xmlGetChildNodesIfOccurs(xmlParsed, 'P', 0))\n",
    "                            q = GetLong(xmlGetChildNodesIfOccurs(xmlParsed, 'Q', 0))\n",
    "                            if p != None and q != None:\n",
    "                                params.extend([q, p])\n",
    "\n",
    "                        qInv = GetLong(xmlGetChildNodesIfOccurs(xmlParsed, 'InverseQ', 0))\n",
    "                        if qInv != None:\n",
    "                            params.append(qInv)\n",
    "                        privateKey = RSA.construct(tuple(params))\n",
    "\n",
    "                        pem = privateKey.exportKey().decode(encoding=\"ascii\")\n",
    "\n",
    "                        fingerprint = sha256(privateKey.export_key(format=\"DER\", pkcs=1)).hexdigest()\n",
    "                        corrFingerprint = sha256(privateKey.public_key().export_key(format=\"DER\")).hexdigest()\n",
    "\n",
    "                        result.append((pem, fingerprint, corrFingerprint))\n",
    "                    elif \"DSA\" in xmlPrivateKey:\n",
    "                        y = GetLong(xmlGetChildNodesIfOccurs(xmlParsed, 'Y', 0))\n",
    "                        g = GetLong(xmlGetChildNodesIfOccurs(xmlParsed, 'G', 0))\n",
    "                        p = GetLong(xmlGetChildNodesIfOccurs(xmlParsed, 'P', 0))\n",
    "                        q = GetLong(xmlGetChildNodesIfOccurs(xmlParsed, 'Q', 0))\n",
    "                        if q != None and y != None and g != None and p != None:\n",
    "                            x = GetLong(xmlGetChildNodesIfOccurs(xmlParsed, 'X', 0))\n",
    "                            if x != None:\n",
    "                                params = (y, g, p, q, x)\n",
    "                            else:\n",
    "                                params = (y, g, p, q)\n",
    "                            privateKey = DSA.construct(params)\n",
    "\n",
    "                            pem = privateKey.exportKey().decode(encoding=\"ascii\")\n",
    "                            fingerprint = sha256(privateKey.export_key(format=\"DER\", pkcs8=False)).hexdigest()\n",
    "                            corrFingerprint = sha256(privateKey.public_key().export_key(format=\"DER\")).hexdigest()\n",
    "\n",
    "                            result.append((pem, fingerprint, corrFingerprint))\n",
    "                        else:\n",
    "                            continue\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                pass\n",
    "    if len(result) == 0:\n",
    "        result.append((\"\", \"\", \"\"))\n",
    "    else:\n",
    "        print(result)\n",
    "    return result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter Functions\n",
    "\n",
    "Functions used to flag matches according to specific filters. Flags are later used to assess the validity of matches."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filters for environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkMultiGroupMatches(row):\n",
    "    checked = []\n",
    "    if len(row['matches_val']) > 0:\n",
    "        valMatchrules = set(row['matches_val'].keys())\n",
    "        varMatchrules = set(row['matches_var'].keys())\n",
    "        secret = row['env_val']\n",
    "        pre = row['env_var']\n",
    "        for r in valMatchrules:\n",
    "            secret = row['matches_val'][r]\n",
    "            if r in multigroupRegexes:\n",
    "                if r in valMatchrules.intersection(varMatchrules):\n",
    "                    pre = row['matches_var'][r]\n",
    "                    checked.append((r, pre, secret))\n",
    "            else:\n",
    "                checked.append((r, \"\", secret))\n",
    "    return checked\n",
    "\n",
    "secretKeywords = ['password', 'key', 'secret']\n",
    "def valPositiveKeywordFilter(row):\n",
    "    for k in secretKeywords:\n",
    "        if k in str.lower(row['env_val']):\n",
    "            return \"varNameKeyword_\" + k\n",
    "    return \"\"\n",
    "\n",
    "valPositiveFilters = [valPositiveKeywordFilter]\n",
    "def applyValPositiveFilters(row):\n",
    "    results = []\n",
    "    for f in valPositiveFilters:\n",
    "        res = f(row)\n",
    "        if res != \"\":\n",
    "            results.append(res)\n",
    "    return results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Group Information\n",
    "\n",
    "Get information of which secret belongs to which group, e.g., Google Cloud Plattform belongs to cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_groups = %sql SELECT name, group FROM matchrule_group mg\n",
    "df_groups = result_groups.DataFrame().set_index(\"name\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get matches from images"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API secret matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql result_api_matches_images <<\n",
    "SELECT DISTINCT\n",
    "    m.rule as rule,\n",
    "    m.registry as registry,\n",
    "    m.repository as repository,\n",
    "    m.layer as layer,\n",
    "    m.match_sha256 as match_sha256,\n",
    "    m.file_name as file_name,\n",
    "    data\n",
    "FROM matches m\n",
    "WHERE m.rule IN (\n",
    "    SELECT name FROM matchrule_group mg WHERE `group` ILIKE '%api%'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_api_matches_images = result_api_matches_images.DataFrame()\n",
    "if len(df_api_matches_images) > 0:\n",
    "    df_api_matches_images['rrl'] = df_api_matches_images[[\"registry\", \"repository\", \"layer\"]].apply(tuple, axis=1)\n",
    "    df_api_matches_images = df_api_matches_images[[\"rule\", \"match_sha256\", \"data\", \"file_name\", \"rrl\"]].groupby([\"rule\", \"match_sha256\", \"data\"]).agg(list).reset_index()\n",
    "    df_api_matches_images['data'] = df_api_matches_images.parallel_apply(lambda row: base64.b64decode(row[\"data\"]), axis = 1)\n",
    "    df_api_matches_images[\"negativeFilter\"] = np.empty((len(df_api_matches_images), 0)).tolist()\n",
    "\n",
    "    trufflehog_gcp_mask = df_api_matches_images[\"rule\"] == \"trufflehog_gcp\"\n",
    "\n",
    "    df_api_matches_images.loc[trufflehog_gcp_mask, 'parsed'] = df_api_matches_images[trufflehog_gcp_mask].parallel_apply(tryParseJSON, axis=1)\n",
    "    df_api_matches_images.loc[trufflehog_gcp_mask, 'secret'] = df_api_matches_images[trufflehog_gcp_mask].parallel_apply(gcpGetSecret, axis=1)\n",
    "    df_api_matches_images.loc[trufflehog_gcp_mask, 'negativeFilter'] += df_api_matches_images[trufflehog_gcp_mask].parallel_apply(lambda row: [\"unparsable\"] if len(row['parsed'])==0 else [], axis=1)\n",
    "\n",
    "    df_api_matches_images.loc[~trufflehog_gcp_mask, 'secret'] = df_api_matches_images[~trufflehog_gcp_mask].parallel_apply(rematch, axis = 1)\n",
    "    df_api_matches_images.loc[~trufflehog_gcp_mask, 'prePart'] = df_api_matches_images[~trufflehog_gcp_mask].parallel_apply(getPrePart, axis = 1)\n",
    "\n",
    "    df_api_matches_images['secret_entropy'] = df_api_matches_images.parallel_apply(lambda row: stringEntropy(list(row[\"secret\"])), axis = 1)\n",
    "    df_api_matches_images['secret_sha256'] = df_api_matches_images.parallel_apply(lambda row: sha256(row[\"secret\"].encode()).hexdigest(), axis = 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Private keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql result_pk_matches_images <<\n",
    "SELECT * FROM (\t\n",
    "\tSELECT DISTINCT registry, repository, layer, match_sha256, file_name, rule FROM matches m\n",
    "\tWHERE rule ILIKE '%private%'\n",
    ") as m\n",
    "LEFT OUTER JOIN (\n",
    "\tSELECT DISTINCT fingerprint as secret_sha256, corrFingerprint, match_sha256 FROM match_findings WHERE type ILIKE '%PRIVATE%'\n",
    ") as mf\n",
    "ON m.match_sha256 == mf.match_sha256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pk_matches_images = result_pk_matches_images.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql result_xml_pk_matches_images <<\n",
    "SELECT DISTINCT registry, repository, layer, match_sha256, data, file_name, rule\n",
    "FROM matches m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_xml_pk_matches_images = result_xml_pk_matches_images.DataFrame()\n",
    "df_xml_pk_matches_images[\"parsed\"] = df_xml_pk_matches_images.apply(lambda row: privKeyPEM(base64.b64decode(row[\"data\"]).decode(errors=\"ignore\")), axis=1)\n",
    "df_xml_pk_matches_images = df_xml_pk_matches_images.explode(\"parsed\").reset_index(drop=True)\n",
    "df_xml_pk_matches_images[['parsed', 'secret_sha256', 'corrFingerprint']] = pd.DataFrame(df_xml_pk_matches_images['parsed'].tolist(), index=df_xml_pk_matches_images.index)\n",
    "add_latex_variable('pkxmlnummatches', df_xml_pk_matches_images[df_xml_pk_matches_images[\"secret_sha256\"] != \"\"][\"secret_sha256\"].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pk_matches_images = df_pk_matches_images.append(df_xml_pk_matches_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pk_matches_images['rrl'] = df_pk_matches_images[[\"registry\", \"repository\", \"layer\"]].parallel_apply(tuple, axis=1)\n",
    "df_pk_matches_images = df_pk_matches_images[[\"rule\", \"match_sha256\", \"file_name\", \"rrl\", \"secret_sha256\", \"corrFingerprint\"]].groupby([\"rule\", \"match_sha256\", \"secret_sha256\", \"corrFingerprint\"]).agg(list).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_matches_images = df_api_matches_images.append(df_pk_matches_images, ignore_index=True).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_matches_images[\"origin\"] = \"image\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get matches from environment variables set in Dockerfiles\n",
    "\n",
    "Also apply filters (see above) to flag matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql result_env_val <<\n",
    "SELECT\n",
    "    registry,\n",
    "    repository,\n",
    "    env_var,\n",
    "    env_val\n",
    "FROM imageconfigs_envval ice\n",
    "WHERE registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_matches_val = result_env_val.DataFrame()\n",
    "df_matches_val.to_pickle(tmp_path_matches_val)\n",
    "df_matches_val_only = df_matches_val[[\"env_val\", \"env_var\"]].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "df_matches_val_only['matches_var'] = df_matches_val_only.parallel_apply(lambda row: match(row['env_var'], retype=\"name\"), axis=1)\n",
    "df_matches_val_only['matches_val'] = df_matches_val_only.parallel_apply(lambda row: match(row['env_val'], retype=\"secret\"), axis=1)\n",
    "df_matches_val_only['checkedMatches'] = df_matches_val_only.parallel_apply(checkMultiGroupMatches, axis=1)\n",
    "df_matches_val_only = df_matches_val_only.explode('checkedMatches')\n",
    "\n",
    "df_matches_val_only = df_matches_val_only[(~df_matches_val_only[\"checkedMatches\"].isnull() & df_matches_val_only[\"checkedMatches\"])]\n",
    "df_matches_val_only[['rule', 'prePart', 'secret']] = pd.DataFrame(df_matches_val_only['checkedMatches'].tolist(), index=df_matches_val_only.index)\n",
    "df_matches_val_only = df_matches_val_only.explode('rule').reset_index(drop=True)\n",
    "df_matches_val_only = df_matches_val_only.drop(df_matches_val_only[df_matches_val_only.rule == 'howbadcanitgit_Gmail'].index)\n",
    "\n",
    "df_matches_val_only['secret_entropy'] = df_matches_val_only.parallel_apply(lambda row: stringEntropy(str(row['secret'])), axis=1)\n",
    "df_matches_val_only['secret_sha256'] = df_matches_val_only.parallel_apply(lambda row: sha256(str(row[\"secret\"]).encode()).hexdigest(), axis = 1)\n",
    "\n",
    "df_matches_val = pd.merge(df_matches_val, df_matches_val_only,  how='right', on=[\"env_val\", \"env_var\"])\n",
    "df_matches_val['match_sha256'] = df_matches_val.parallel_apply(lambda row: sha256(row[\"env_val\"].encode()).hexdigest(), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_num_matches_val = df_matches_val[[\"match_sha256\", \"rule\"]]\n",
    "df_num_matches_val = df_num_matches_val.join(df_groups, on=\"rule\")\n",
    "df_num_matches_val = df_num_matches_val[[\"match_sha256\", \"group\"]].rename(columns={\"match_sha256\": \"num_matches\"})\n",
    "df_num_matches_val[\"num_distinct_matches\"] = df_num_matches_val[\"num_matches\"]\n",
    "df_num_matches_val = df_num_matches_val.groupby(\"group\").agg({\"num_matches\": \"count\", \"num_distinct_matches\": \"nunique\"})\n",
    "\n",
    "df_matches_val['layer'] = \"none\"\n",
    "df_matches_val['rrl'] = df_matches_val[[\"registry\", \"repository\", \"layer\"]].apply(tuple, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_matches_val = df_matches_val[[\"rrl\", \"env_val\", \"env_var\", \"rule\", \"match_sha256\", \"secret_sha256\", \"secret\"]].groupby([\"rule\", \"env_val\", \"env_var\", \"match_sha256\", \"secret\", \"secret_sha256\"]).agg(list).reset_index()\n",
    "df_matches_val[\"origin\"] = \"val\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine dataframes with matches from files and environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_matches = pd.concat([df_matches_val, df_matches_images], ignore_index=True).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_matches = df_matches.join(df_groups, on=\"rule\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply filter to API secrets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters to generate ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate (4,7)-character ngrams\n",
    "ngramMin = 4\n",
    "ngramMax = 7\n",
    "\n",
    "# Later filter out matches containing ngrams that occur 29 times more often than the mean over all ngrams\n",
    "frequencyNgramsTimeFactor = 29"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix parts in secrets\n",
    "from nltk import everygrams\n",
    "from itertools import chain\n",
    "\n",
    "fixPartsToExclude = [\"----- BEGIN PRIVATE KEY -----\", \"----- END PRIVATE KEY -----\", \"EAACEdEose0cBA\", \"AIza\", \".apps.googleusercontent.com\", \"sk_live_\", \"rk_live_\", \"sq0atp-\", \"sq0csp-\", \"access_token$production$\", \"amzn.mws.\", \"key-\", \"AKIA\", \"auth_provider_x509_cert_url\", \"glpat\", \"ghp\", \"gho\", \"ghu\", \"ghs\", \"ghr\", \"LTAI\", \"aio\", \"https\", \".webhook.office.com/webhookb2\", \"IncomingWebhook\"]\n",
    "fixPartsToExcludeNgrams = []\n",
    "for fp in fixPartsToExclude:\n",
    "    fixPartsToExcludeNgrams.extend([''.join(x) for x in list(everygrams(fp, min_len=4, max_len=7))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_matches_test = df_matches[~df_matches[\"secret\"].isnull()].copy()\n",
    "df_matches_test[\"ngrams\"] = df_matches_test.swifter.apply(lambda row: everygrams(row[\"secret\"], min_len=ngramMin, max_len=ngramMax), axis=1)\n",
    "grams = pd.Series(list(chain.from_iterable(df_matches_test[~df_matches_test[\"rule\"].str.contains(\"private\")]['ngrams'])), name=\"occurrences\")\n",
    "frequency = grams.swifter.apply(''.join).value_counts().to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequencyExcluded = frequency[~frequency.index.isin(fixPartsToExcludeNgrams)]\n",
    "frequencyExcluded = frequencyExcluded[~((frequencyExcluded.index.str.count('\\d')) >= (frequencyExcluded.index.str.len()/2))]\n",
    "mostFrequentNgrams = frequencyExcluded[frequencyExcluded[\"occurrences\"] > frequencyNgramsTimeFactor*frequencyExcluded[\"occurrences\"].mean()]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply further filtering"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter out matches containing specific keywords (and most frequent ngrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywordsSecret = mostFrequentNgrams.index.to_list()\n",
    "keywordsPrePart = [\"sha256\", \"sha512\"]\n",
    "exceptionsSecret = {}\n",
    "exceptionsPrePart = {\n",
    "    \"trufflehog_heroku\": [\"*\"]\n",
    "}\n",
    "def filterKeyWords(row):\n",
    "    for k in keywordsSecret:\n",
    "        if row[\"rule\"] in exceptionsSecret:\n",
    "            if k in exceptionsSecret[row[\"rule\"]] or \"*\" in exceptionsSecret[row[\"rule\"]]:\n",
    "                continue\n",
    "        if k in str.lower(str(row[\"secret\"])):\n",
    "            return \"keywordsecret_\" + k\n",
    "\n",
    "    for k in keywordsPrePart:\n",
    "        if row[\"rule\"] in exceptionsPrePart:\n",
    "            if k in exceptionsPrePart[row[\"rule\"]] or \"*\" in exceptionsPrePart[row[\"rule\"]]:\n",
    "                continue\n",
    "        if k in str.lower(str(row[\"prePart\"])):\n",
    "            return \"keywordPrePart_\" + k\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "excludedCharsFromSequenceRule = {\n",
    "    \"trufflehog_gcp\": [\"-\"]\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter sequences\n",
    "\n",
    "##### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exceptionsSequence = [\" \"]\n",
    "sequenceLimitNotSame = 4\n",
    "sequenceLimitSame = 3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterSequences(row):\n",
    "    string = str(row[\"secret\"])\n",
    "\n",
    "    exclude = exceptionsSequence\n",
    "    if row[\"rule\"] in excludedCharsFromSequenceRule:\n",
    "        exclude.extend(excludedCharsFromSequenceRule[row[\"rule\"]])\n",
    "\n",
    "    excludeOrd = [ord(c) for c in exclude]\n",
    "\n",
    "    last = -1\n",
    "    num_sequence_asc = 0\n",
    "    num_sequence_desc = 0\n",
    "    num_sequence_same = 0\n",
    "    for letter in string:\n",
    "        cur = ord(letter)\n",
    "        if cur not in excludeOrd:\n",
    "            num_sequence_same = num_sequence_same + 1 if last == cur else 0\n",
    "            num_sequence_desc = num_sequence_desc + 1 if last-1 == cur else 0\n",
    "            num_sequence_asc = num_sequence_asc + 1 if last+1 == cur else 0\n",
    "            last = cur\n",
    "\n",
    "            if num_sequence_same >= sequenceLimitSame:\n",
    "                return \"sequence_same\"\n",
    "            elif num_sequence_desc >= sequenceLimitNotSame:\n",
    "                return \"sequence_desc\"\n",
    "            elif num_sequence_asc >= sequenceLimitNotSame:\n",
    "                return \"sequence_asc\"\n",
    "        else:\n",
    "            last = -1\n",
    "            num_sequence_asc = 0\n",
    "            num_sequence_desc = 0\n",
    "            num_sequence_same = 0\n",
    "    else:\n",
    "        return \"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combined filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filters = [filterKeyWords, filterSequences]\n",
    "def filter(row):\n",
    "    results = []\n",
    "    for f in filters:\n",
    "        result = f(row)\n",
    "        if result != \"\":\n",
    "            results.append(result)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setNaToEmptyList(df, column):\n",
    "    rows = np.where(pd.isnull(df[column]))\n",
    "    \n",
    "    for r in rows[0]:\n",
    "        df.loc[r, column] = [[]]\n",
    "\n",
    "df_matches[\"file_name\"] = df_matches[\"file_name\"].swifter.apply(lambda d: d if isinstance(d, list) else [])\n",
    "df_matches[\"negativeFilter\"] = df_matches[\"negativeFilter\"].swifter.apply(lambda d: d if isinstance(d, list) else [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkMask = (df_matches[\"group\"] == \"private_key\")\n",
    "df_matches.loc[~pkMask, 'negativeFilter'] += df_matches[~pkMask].parallel_apply(filter, axis = 1)\n",
    "df_matches.loc[pkMask, \"negativeFilter\"] += df_matches[pkMask].parallel_apply(lambda row: [] if row[\"secret_sha256\"] != \"\" else [\"unparsable\"], axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter private keys"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering based on kompromat (https://github.com/SecurityFail/kompromat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkMask = (df_matches[\"group\"] == \"private_key\")\n",
    "df_matches_pk_knownkompromat = pd.DataFrame()\n",
    "\n",
    "validMatches = df_matches[((pkMask) & (df_matches[\"negativeFilter\"].map(len) == 0) & (~df_matches[\"secret_sha256\"].isnull()))][\"secret_sha256\"].tolist()\n",
    "n = 3000\n",
    "for i in range(0, len(validMatches), n):\n",
    "    curValMatchesString = '\\'' + '\\',\\''.join(validMatches[i:i + n]) + '\\''\n",
    "    result_valid_matches = %sql SELECT metaId, infos, fingerprint as secret_sha256 FROM kompromat k WHERE k.fingerprint IN ({curValMatchesString})\n",
    "    df_matches_pk_knownkompromat = df_matches_pk_knownkompromat.append(result_valid_matches.DataFrame()) \n",
    "\n",
    "df_matches_pk_knownkompromat[[\"metaIdSplit1\", \"metaIdSplit2\", \"metaIdSplit3\", \"metaIdSplit4\", \"metaIdSplit5\"]]= df_matches_pk_knownkompromat[\"metaId\"].str.split(\"/\", n = 4, expand = True)\n",
    "df_matches_pk_knownkompromat = df_matches_pk_knownkompromat[[\"metaIdSplit2\", \"metaIdSplit3\", \"secret_sha256\", \"infos\"]]\n",
    "\n",
    "# This is what we filter als \"invalid\"\n",
    "kompromatFilter = [\"rfc\", \"softwaretests\", \"testvectors\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get number of secrets in kompromat\n",
    "##filtered (testkeys)\n",
    "df_matches_pk_knownkompromat_unique_filtered = df_matches_pk_knownkompromat[[\"metaIdSplit2\", \"metaIdSplit3\", \"secret_sha256\"]]\n",
    "df_matches_pk_knownkompromat_unique_filtered = df_matches_pk_knownkompromat_unique_filtered[df_matches_pk_knownkompromat_unique_filtered[\"metaIdSplit2\"].isin(kompromatFilter)]\n",
    "\n",
    "df_matches_pk_knownkompromat_unique_grouped1_filtered = df_matches_pk_knownkompromat_unique_filtered.groupby(\"metaIdSplit2\").nunique()\n",
    "df_matches_pk_knownkompromat_unique_grouped2_filtered = df_matches_pk_knownkompromat_unique_filtered.groupby([\"metaIdSplit2\", \"metaIdSplit3\"]).nunique().reset_index(level=0)\n",
    "\n",
    "df_matches_pk_knownkompromat_unique_grouped2_filtered = df_matches_pk_knownkompromat_unique_grouped2_filtered. \\\n",
    "    loc[df_matches_pk_knownkompromat_unique_grouped2_filtered.groupby([\"metaIdSplit2\"])[\"secret_sha256\"].idxmax()]. \\\n",
    "        reset_index().set_index(\"metaIdSplit2\").rename(columns={\"metaIdSplit3\": \"service\", \"secret_sha256\": \"numdistinct\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkMaskParsable = ((df_matches[\"group\"] == \"private_key\") & (df_matches[\"negativeFilter\"].map(len) == 0))\n",
    "\n",
    "df_matches_pk_knownkompromat_unique_filtered_fingerprints = df_matches_pk_knownkompromat_unique_filtered[df_matches_pk_knownkompromat_unique_filtered[\"metaIdSplit2\"].isin(kompromatFilter)].drop_duplicates()[\"secret_sha256\"]\n",
    "df_matches.loc[pkMaskParsable, \"negativeFilter\"] += df_matches[pkMaskParsable].parallel_apply(lambda row: [\"kompromat\"] if row[\"secret_sha256\"] in df_matches_pk_knownkompromat_unique_filtered_fingerprints.to_list() else [], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filterRules = [\"trufflehog_{}\".format(x) for x in [\"azure_old\", \"ibmclouduserkey\", \"digitaloceantoken\", \"gitlab\", \"currencycloud\", \"openuv\", \"netlify\", \"coinbase\", \"bitfenix\", \"accuweather\", \"wepay\", \"tomtom\", \"ticketmaster\", \"paymongo\", \"paymoapp\", \"paydirtapp\", \"facebookkey\"] + [\"howbadcanitgit_twitter\"]]\n",
    "df_matches[\"negativeFilter\"] += df_matches.swifter.apply(lambda row: [\"rule\"] if row[\"rule\"] in filterRules else [], axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter all types"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter by filepath"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define filepaths to be flagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefixes = [\"usr\\/lib\", \"var\\/lib\", \"lib\", \"var\\/cache\", \"usr\\/local\\/share\", \"usr\\/share\\/doc\", \"etc\\/openvpn\\/hidemyass\", \"etc\\/openvpn\\/vpnbook\", \"etc\\/openvpn\\/proxpn\", \"etc\\/openvpn\\/expressvpn\", \"etc\\/openvpn\\/purevpn\", \"etc\\/openvpn\\/freevpn\", \"etc\\/openvpn\\/elastictunnel\", \"etc\\/openvpn\\/anonvpn\", \"etc\\/openvpn\\/froot\", \"etc\\/openvpn\\/cactusvpn\", \"ovpn4\", \"vpngate\\/config\"]\n",
    "prefixesAndTest = [\"usr\\/share\\/java\"]\n",
    "contains = [\"go\\/pkg\\/mod\\/cache\", \"hideipvpn.com_\", \"\\.gradle\\/caches\", \"node\\_modules\", \"\\.gradle\\/wrapper\", \"\\.composer\\/cache\", \"\\.cache\\/bazel\", \"lib\\/ruby\\/gems\", \"vendor\\/bundle\\/jruby\\/.*\\/gems\", \"azure\\/cli\\/.*\\/tests\", \"php\\/test\\/ssh2\\/tests\", \"google\\-cloud\\-sdk\\/platform\", \"usr\\/local\\/lib\", \"flutter\\/.*\\/lib\",  \"flutter\\/.*\\/cache\", \"\\.cargo\\/registry\", \"flutter\\/.*\\/test\", \"ffead\\-.*\\/.*\\/test\", \"elasticsearch\\/plugins\", \"python.*\\/.*-packages\", \"julia\\/packages\", \"strongswan-.*\\/testing\", \"\\.cache\\/pip\", \"\\.cache\\/helm\", \"\\.cache\\/yarn\", \"cache\\/\\.ivy2\", \"go\\-dockerclient\\/testing\", \"conda\\/pkgs\", \"opkg\\-lists\", \".linuxbrew\", \".bundle\\/cache\", \"\\.cache\\/luarocks\", \"metasploit\", \"esp\\-idf\\/examples\", \"\\.vim\\/bundle\", \"\\.vim\\/plugged\", \"android\\-sdk\\/platform\\-tools\", \"\\.cpanm\\/work\", \"nixpkgs\\/pkgs\", \"swagger\\-codegen\\/modules\", \"maven\\/ref\", \"nrfxlib\\/openthread\\/lib\", \"\\.cargo\\/registry\\/src\", \"\\.meteor\\/package\\-metadata\", \"\\.npm\\/_cacache\", \"android\\/sdk\\/emulator\", \"android\\-sdk\\-linux\", \"metadata\\/md5\\-cache\", \"rails\\/bifrost\\/log\", \"\\.cache\\/heroku\\/yarn\", \"mssql\\/data\", \"repo\\/state\\.cache\", \"android\\-sdk\\/emulator\\/qemu\\/linux\", \"kafka\\/logs\\/server\\.log\", \"\\.cache\\/go\\-build\", \"build\\-helpers\\/patches\\/notes\", \"\\.platformio\\/packages\"]\n",
    "containsAndTest = [\"org\\.eclipse\\.paho\", \"paho\\.mqtt\", \"cassandra\", \"golang\\.org\\/x\", \"contrib\"]\n",
    "suffixes = [\"RECORD\", \"packageinfo\", \"HEAD\", \"FETCH_HEAD\", \"\\.yarn\\-metadata\\.json\", \"version\\.txt\", \"zookeeper\\.out\", \"VERSION\\_BUILD\\.json\"] # [\"\\.svg\", \"md5sums\", \"versions\", ]\n",
    "filenameContains = [] #[\"manifest\"]\n",
    "filetypes = [\"md\", \"mdf\", \"mfa\", \"maf\", \"fasta\", \"fastq\", \"fq\", \"fa\", \"seed\", \"ibd\", \"dbf\", \"db\", \"sto\", \"spdx\\.json\", \"tga\", \"xwd\", \"blm\\.lm\", \"ebuild\", \"fdt\", \"ttf\", \"jar\\.pack\", \"sqlite\", \"sqlite3\", \"version\", \"yuv\", \"wt\", \"miff\", \"lm\", \"hrl\", \"cfs\", \"bsp\", \"avi\", \"bag\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regexes = []\n",
    "regexes.extend([\"^(\\/)*{}\".format(p) for p in prefixes])\n",
    "regexes.extend([\"^(\\/)*{}.*test\".format(p) for p in prefixesAndTest])\n",
    "regexes.extend([\"{}\".format(c) for c in contains])\n",
    "regexes.extend([\"{}.*test\".format(c) for c in containsAndTest])\n",
    "regexes.extend([\"{}$\".format(s) for s in suffixes])\n",
    "regexes.extend([\"{}[^\\/]*$\".format(f) for f in filenameContains])\n",
    "regexes.extend([\"{}$\".format(f) for f in filetypes])\n",
    "regexes.extend([\"{}.gz$\".format(f) for f in filetypes])\n",
    "\n",
    "cRegex = re.compile('|'.join(regexes))\n",
    "def filterPathRegex(row):\n",
    "    for p in row[\"file_name\"]:\n",
    "        file_name = str(p)\n",
    "        if cRegex.search(file_name):\n",
    "            return [\"filename\"]\n",
    "    return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_matches['negativeFilter'] += df_matches.parallel_apply(filterPathRegex, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanFilePaths(fp, cleanprefixes):\n",
    "    result = fp\n",
    "    for cp in cleanprefixes:\n",
    "        if fp.startswith(cp):\n",
    "            result = result[len(cp):]\n",
    "    return result\n",
    "\n",
    "pkMask = (df_matches[\"group\"] == \"private_key\")\n",
    "df_matches_pk_with_filepaths_tmp = df_matches[pkMask].explode(\"file_name\").reset_index(drop=True)\n",
    "df_matches_pk_with_filepaths_tmp = df_matches_pk_with_filepaths_tmp[~df_matches_pk_with_filepaths_tmp[\"file_name\"].isnull()]\n",
    "df_matches_pk_with_filepaths_tmp[\"file_name\"] = df_matches_pk_with_filepaths_tmp.parallel_apply(lambda row: cleanFilePaths(row[\"file_name\"], ['./', '/']), axis=1)\n",
    "df_matches_pk_with_filepaths_tmp[\"iskompromat\"] = df_matches_pk_with_filepaths_tmp.parallel_apply(lambda row: True if \"kompromat\" in row[\"negativeFilter\"] else False, axis=1)\n",
    "\n",
    "levels = df_matches_pk_with_filepaths_tmp[\"file_name\"].str.count('/').max()+1\n",
    "splits = [\"fileNameSplit{}\".format(l) for l in range(1,levels+1)]\n",
    "df_matches_pk_with_filepaths_tmp[splits] = df_matches_pk_with_filepaths_tmp[\"file_name\"].str.split(\"/\", n = levels-1, expand = True)\n",
    "df_matches_pk_with_filepaths_tmp[splits] = df_matches_pk_with_filepaths_tmp[splits].fillna(\"\")\n",
    "\n",
    "splitplus = \"fileNameSplit{}\".format(len(splits)+1)\n",
    "\n",
    "df_matches_pk_with_filepaths_tmp[splitplus] = \"\"\n",
    "df_matches_pk_with_filepaths_tmp = df_matches_pk_with_filepaths_tmp.rename(columns={\"secret_sha256\":\"numkompromat\"})\n",
    "df_matches_pk_with_filepaths_tmp = df_matches_pk_with_filepaths_tmp[splits + [splitplus] + [\"iskompromat\", \"numkompromat\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regeneratePaths(df, subfolders=False):\n",
    "    if len(df) > 0:\n",
    "        return [(p.rstrip('/'), subfolders) for p in df.reset_index().drop([\"numkompromatTrue\", \"numkompromatFalse\"], axis=1).agg('/'.join, axis=1).to_list()]\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "def generatePathsKompromat(df):\n",
    "    result = []\n",
    "\n",
    "    cols = list(df.columns)\n",
    "\n",
    "    curdf = df\n",
    "    curdf = curdf.groupby(cols[0:len(cols)-1]).nunique().unstack(level=-1, fill_value=0)\n",
    "    curdf.columns = [f\"{x}{y}\" for x, y in curdf.columns.to_flat_index()]\n",
    "\n",
    "    levels = list(curdf.index.names)\n",
    "    \n",
    "    curdf = curdf.iloc[curdf.index.get_level_values(len(levels)-2) != \"\"]\n",
    "\n",
    "    firstocc = curdf[(curdf.index.get_level_values(len(levels)-1) == \"\")]\n",
    "    kfirstocc = firstocc.groupby(level=levels[0:len(levels)-2]).sum()\n",
    "    result.extend(regeneratePaths(kfirstocc[kfirstocc[\"numkompromatTrue\"] > 0]))\n",
    "\n",
    "    ongoingocc = curdf[(curdf.index.get_level_values(len(levels)-1) != \"\")]\n",
    "    if len(ongoingocc) > 0:\n",
    "        kongoingocc = ongoingocc.groupby(level=levels[0:len(levels)-2]).sum()\n",
    "        paths = kongoingocc[0.5 * kongoingocc[\"numkompromatTrue\"] > kongoingocc[\"numkompromatFalse\"]]\n",
    "        result.extend(regeneratePaths(paths, subfolders=True))\n",
    "\n",
    "    if len(cols) > 7:\n",
    "        cols = cols[0:len(cols)-3] + cols[len(cols)-2:]\n",
    "        nexdf = df[cols]\n",
    "        result.extend(generatePathsKompromat(nexdf))\n",
    "\n",
    "    return result\n",
    "\n",
    "filteredPaths = generatePathsKompromat(df_matches_pk_with_filepaths_tmp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkPathsKompromat(row):\n",
    "    res = []\n",
    "    for fp in row[\"file_name\"]:\n",
    "        fpClean = fp.lstrip('/')\n",
    "        for ffp in filteredPaths:\n",
    "            if fpClean.startswith(ffp[0]):\n",
    "                if ffp[1]:\n",
    "                    res.append(ffp[0])\n",
    "                else:\n",
    "                    rest = fpClean[len(ffp[0])+1:]\n",
    "                    if not '/' in rest:\n",
    "                        res.append(ffp[0])\n",
    "    return res\n",
    "\n",
    "df_matches.loc[pkMask, \"matchingfpkompromat\"] = df_matches[pkMask].parallel_apply(checkPathsKompromat, axis=1)\n",
    "df_matches.loc[pkMask, \"negativeFilter\"] += df_matches[pkMask].parallel_apply(lambda row: [\"file_path\"] if len(row[\"matchingfpkompromat\"]) else [], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_matches.to_pickle('df_matches.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_matches_filepathskompromat = df_matches[[\"matchingfpkompromat\", \"secret_sha256\"]].explode(\"matchingfpkompromat\").groupby(\"matchingfpkompromat\").nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_most_filtered_paths = df_matches_filepathskompromat.sort_values(by=\"secret_sha256\", ascending=False).head(10).reset_index()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
